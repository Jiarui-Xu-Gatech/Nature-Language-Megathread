{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnnwupedFEIV"
      },
      "source": [
        "### Part 0. Google Colab Setup"
      ],
      "id": "JnnwupedFEIV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiFjTT4jFEIW"
      },
      "source": [
        "Welcome to the first full programming project for CS 7650! If you're new to Google Colab we recommend looking at [this](https://colab.research.google.com/notebooks/basic_features_overview.ipynb) intro notebook before getting started with this project. In short, Colab is a Jupyter notebook environment that runs in the cloud, it's recommended for all of the programming projects in this course due to its availability, ease of use, and hardware accessibility. Some features that you may find especially useful are on the left hand side, these being:\n",
        "\n",
        "*   Table of contents: displays the sections of the notebook made using text cells\n",
        "*   Variables: useful for debugging and see current values of variables\n",
        "*   Files: useful or uploading or downloading any files you upload to Colab or write while working on the projects\n",
        "\n",
        "\n",
        "\n",
        "**To begin this project, make a copy of this notebook and save it to your local drive so that you can edit it.**\n",
        "\n",
        "\n",
        "If you want GPU's (which will improve training times), you can always change your instance type to GPU by going to Runtime -> Change runtime type -> Hardware accelerator.\n",
        "\n",
        "If you're new to PyTorch, or simply want a refresher, we recommend you start by looking through these [Introduction to PyTorch](https://cocoxu.github.io/CS4650_spring2022/slides/PyTorch_tutorial.pdf) slides and this interactive [PyTorch Basics notebook](http://bit.ly/pytorchbasics). Additionally, this [Text Sentiment](http://bit.ly/pytorchexample) notebook will provide some insight into working with PyTorch for NLP specific problems. "
      ],
      "id": "SiFjTT4jFEIW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRdPqeMMFEIY"
      },
      "source": [
        "### Part 1. Loading and Preprocessing Data [10 points]\n",
        "The following cell loads the OnionOrNot dataset, and tokenizes each data item"
      ],
      "id": "ZRdPqeMMFEIY"
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://raw.githubusercontent.com/lukefeilberg/onion/master/OnionOrNot.csv > OnionOrNot.csv"
      ],
      "metadata": {
        "id": "YmV_uknBJA-o",
        "outputId": "f8923150-3c33-4a6f-9675-50a9d05ef091",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "id": "YmV_uknBJA-o",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 1903k  100 1903k    0     0  8310k      0 --:--:-- --:--:-- --:--:-- 8274k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "L3DkMDu7FEIZ"
      },
      "outputs": [],
      "source": [
        "# DO NOT MODIFY #\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "# this is how we select a GPU if it's avalible on your computer or in the Colab environment.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "id": "L3DkMDu7FEIZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part 1.1 Preprocessing definitions:\n",
        "The following cell define some methods to clean the dataset. Do not edit it, but feel free to take a look at some of the operations it's doing. \n"
      ],
      "metadata": {
        "id": "0Fulh0MZ8y8b"
      },
      "id": "0Fulh0MZ8y8b"
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THIS BLOCK\n",
        "# example code taken from fast-bert\n",
        "\n",
        "import re\n",
        "import html\n",
        "\n",
        "def spec_add_spaces(t: str) -> str:\n",
        "    \"Add spaces around / and # in `t`. \\n\"\n",
        "    return re.sub(r\"([/#\\n])\", r\" \\1 \", t)\n",
        "\n",
        "def rm_useless_spaces(t: str) -> str:\n",
        "    \"Remove multiple spaces in `t`.\"\n",
        "    return re.sub(\" {2,}\", \" \", t)\n",
        "\n",
        "def replace_multi_newline(t: str) -> str:\n",
        "    return re.sub(r\"(\\n(\\s)*){2,}\", \"\\n\", t)\n",
        "\n",
        "def fix_html(x: str) -> str:\n",
        "    \"List of replacements from html strings in `x`.\"\n",
        "    re1 = re.compile(r\"  +\")\n",
        "    x = (\n",
        "        x.replace(\"#39;\", \"'\")\n",
        "        .replace(\"amp;\", \"&\")\n",
        "        .replace(\"#146;\", \"'\")\n",
        "        .replace(\"nbsp;\", \" \")\n",
        "        .replace(\"#36;\", \"$\")\n",
        "        .replace(\"\\\\n\", \"\\n\")\n",
        "        .replace(\"quot;\", \"'\")\n",
        "        .replace(\"<br />\", \"\\n\")\n",
        "        .replace('\\\\\"', '\"')\n",
        "        .replace(\" @.@ \", \".\")\n",
        "        .replace(\" @-@ \", \"-\")\n",
        "        .replace(\" @,@ \", \",\")\n",
        "        .replace(\"\\\\\", \" \\\\ \")\n",
        "    )\n",
        "    return re1.sub(\" \", html.unescape(x))\n",
        "\n",
        "def clean_text(input_text):\n",
        "    text = fix_html(input_text)\n",
        "    text = replace_multi_newline(text)\n",
        "    text = spec_add_spaces(text)\n",
        "    text = rm_useless_spaces(text)\n",
        "    text = text.strip()\n",
        "    return text"
      ],
      "metadata": {
        "id": "ctNnE1Ui8oKw"
      },
      "id": "ctNnE1Ui8oKw",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part 1.2 Clean the data using the methods above and tokenize it using NLTK"
      ],
      "metadata": {
        "id": "MiUlTSBB9Wx6"
      },
      "id": "MiUlTSBB9Wx6"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vqtdrhF8FEIZ",
        "outputId": "a77deff4-3e0e-4624-c37e-2db2b6d71dc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from tqdm import tqdm\n",
        "\n",
        "nltk.download('punkt')\n",
        "df              = pd.read_csv(\"OnionOrNot.csv\")\n",
        "df[\"tokenized\"] = df[\"text\"].apply(lambda x: nltk.word_tokenize(clean_text(x.lower())))"
      ],
      "id": "vqtdrhF8FEIZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBBdVOYxFEIa"
      },
      "source": [
        "Here's what the dataset looks like. You can index into specific rows with pandas, and try to guess some of these yourself :). If you're unfamiliar with pandas, it's a extremely useful and popular library for data analysis and manipulation. You can find their documentation [here](https://pandas.pydata.org/docs/). \n",
        "\n",
        "Pandas primary data structure is a DataFrame. The following cell will print out the basic information of this structure, including the labeled axes (both columns and rows) as well as show you what the first n (default=5) rows look like"
      ],
      "id": "qBBdVOYxFEIa"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "sJjScqV3FEIb",
        "outputId": "1dee1e00-0773-4ae1-c223-28f2562f9a29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  label  \\\n",
              "0  Entire Facebook Staff Laughs As Man Tightens P...      1   \n",
              "1  Muslim Woman Denied Soda Can for Fear She Coul...      0   \n",
              "2  Bold Move: Hulu Has Announced That They’re Gon...      1   \n",
              "3  Despondent Jeff Bezos Realizes He’ll Have To W...      1   \n",
              "4  For men looking for great single women, online...      1   \n",
              "\n",
              "                                           tokenized  \n",
              "0  [entire, facebook, staff, laughs, as, man, tig...  \n",
              "1  [muslim, woman, denied, soda, can, for, fear, ...  \n",
              "2  [bold, move, :, hulu, has, announced, that, th...  \n",
              "3  [despondent, jeff, bezos, realizes, he, ’, ll,...  \n",
              "4  [for, men, looking, for, great, single, women,...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f9733eee-aa48-47e1-b4ea-d13d30f3541c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>tokenized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Entire Facebook Staff Laughs As Man Tightens P...</td>\n",
              "      <td>1</td>\n",
              "      <td>[entire, facebook, staff, laughs, as, man, tig...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Muslim Woman Denied Soda Can for Fear She Coul...</td>\n",
              "      <td>0</td>\n",
              "      <td>[muslim, woman, denied, soda, can, for, fear, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Bold Move: Hulu Has Announced That They’re Gon...</td>\n",
              "      <td>1</td>\n",
              "      <td>[bold, move, :, hulu, has, announced, that, th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Despondent Jeff Bezos Realizes He’ll Have To W...</td>\n",
              "      <td>1</td>\n",
              "      <td>[despondent, jeff, bezos, realizes, he, ’, ll,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>For men looking for great single women, online...</td>\n",
              "      <td>1</td>\n",
              "      <td>[for, men, looking, for, great, single, women,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f9733eee-aa48-47e1-b4ea-d13d30f3541c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f9733eee-aa48-47e1-b4ea-d13d30f3541c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f9733eee-aa48-47e1-b4ea-d13d30f3541c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "df.head()"
      ],
      "id": "sJjScqV3FEIb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "DataFrames can be indexed using [.iloc\\[ ]](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html), this primarily uses interger based indexing and supports a single integer (i.e. 42), a list of integers (i.e. [1, 5, 42]), or even a slice (i.e. 7:42). "
      ],
      "metadata": {
        "id": "D9b4W9z1XhgS"
      },
      "id": "D9b4W9z1XhgS"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ntm8laX6FEIb",
        "outputId": "be566336-7a87-4e2b-a8af-7f7b05d84e32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "text         Customers continued to wait at drive-thru even...\n",
              "label                                                        0\n",
              "tokenized    [customers, continued, to, wait, at, drive-thr...\n",
              "Name: 42, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "df.iloc[42]"
      ],
      "id": "Ntm8laX6FEIb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part 1.3 Split the dataset into training, validation, and testing"
      ],
      "metadata": {
        "id": "TQVT6HUA9htQ"
      },
      "id": "TQVT6HUA9htQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDI72x8XFEIc"
      },
      "source": [
        "Now that we've loaded this dataset, we need to split the data into train, validation, and test sets. A good explanation of why we need these different sets can be found in subsection 2.2.5 of [Eisenstein](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf) but at the end it comes down to having a trustworthy and generalized model. The validation (sometimes called a development or tuning) set is used to help choose hyperparameters for our model, whereas the training set is used to fit the learned parameters (weights and biases) to the task. The test set is used to provide a final unbiased evaluation of our trained model, hopefully providing some insight into how it would actually do in production. Each of these sets should be disjoint from the others, to prevent any \"peeking\" that could unfairly influence our understanding of the model's accuracy. \n",
        "\n",
        "In addition to these different sets of data, we also need to create a vocab map for words in our Onion dataset, which will map tokens to numbers. This will be useful later, since torch PyTorch use tensors of sequences of numbers as inputs. **Go to the following cell, and fill out split_train_val_test and generate_vocab_map.**"
      ],
      "id": "GDI72x8XFEIc"
    },
    {
      "cell_type": "code",
      "source": [
        "# BEGIN - DO NOT CHANGE THESE IMPORTS/CONSTANTS OR IMPORT ADDITIONAL PACKAGES.\n",
        "from collections import Counter\n",
        "PADDING_VALUE = 0\n",
        "UNK_VALUE     = 1\n",
        "# END - DO NOT CHANGE THESE IMPORTS/CONSTANTS OR IMPORT ADDITIONAL PACKAGES.\n",
        "\n",
        "\n",
        "# split_train_val_test\n",
        "# This method takes a dataframe and splits it into train/val/test splits.\n",
        "# It uses the props argument to split the dataset appropriately.\n",
        "#\n",
        "# args:\n",
        "# df - the entire dataset DataFrame \n",
        "# props - proportions for each split in the order of [train, validation, test]. \n",
        "#         the last value of the props array is repetitive, but we've kept it for clarity.\n",
        "#\n",
        "# returns: \n",
        "# train DataFrame, val DataFrame, test DataFrame\n",
        "#\n",
        "def split_train_val_test(df, props=[.8, .1, .1]):\n",
        "    assert round(sum(props), 2) == 1 and len(props) >= 2\n",
        "    train_df, test_df, val_df = None, None, None\n",
        "    ## YOUR CODE STARTS HERE (~3-5 lines of code) ##\n",
        "    # hint: you can use df.iloc to slice into specific indexes or ranges.\n",
        "    if len(props)<3:\n",
        "      props.append(0)\n",
        "    train_df=df.iloc[:int(props[0]*df.shape[0])]\n",
        "    val_df=df.iloc[int(props[0]*df.shape[0]):int(props[0]*df.shape[0])+int(props[1]*df.shape[0])]\n",
        "    test_df=df.iloc[int(props[1]*df.shape[0]):int(props[1]*df.shape[0])+int(props[2]*df.shape[0])]\n",
        "    \n",
        "    ## YOUR CODE ENDS HERE ##\n",
        "    \n",
        "    return train_df, val_df, test_df\n",
        "\n",
        "# generate_vocab_map\n",
        "# This method takes a dataframe and builds a vocabulary to unique number map.\n",
        "# It uses the cutoff argument to remove rare words occuring <= cutoff times. \n",
        "# *NOTE*: \"\" and \"UNK\" are reserved tokens in our vocab that will be useful\n",
        "# later. You'll also find the Counter imported for you to be useful as well.\n",
        "# \n",
        "# args:\n",
        "# df     - the entire dataset this mapping is built from \n",
        "# cutoff - we exclude words from the vocab that appear less than or\n",
        "#          eq to cutoff\n",
        "#\n",
        "# returns: \n",
        "# vocab - dict[str] = int\n",
        "#         In vocab, each str is a unique token, and each dict[str] is a \n",
        "#         unique integer ID. Only elements that appear > cutoff times appear\n",
        "#         in vocab.\n",
        "#\n",
        "# reversed_vocab - dict[int] = str\n",
        "#                  A reversed version of vocab, which allows us to retrieve \n",
        "#                  words given their unique integer ID. This map will \n",
        "#                  allow us to \"decode\" integer sequences we'll encode using\n",
        "#                  vocab!\n",
        "# \n",
        "def generate_vocab_map(df, cutoff=2):\n",
        "    vocab          = {\"\": PADDING_VALUE, \"UNK\": UNK_VALUE}\n",
        "    reversed_vocab = None\n",
        "    \n",
        "    ## YOUR CODE STARTS HERE (~5-15 lines of code) ##\n",
        "    # hint: start by iterating over df[\"tokenized\"]\n",
        "    reversed_vocab={}\n",
        "    result = Counter([item for sublist in df[\"tokenized\"].to_list() for item in sublist])\n",
        "    ID=2\n",
        "    for item in result.keys():\n",
        "      if result[item]>cutoff:\n",
        "        vocab[item]=ID\n",
        "        reversed_vocab[ID]=item\n",
        "        ID+=1\n",
        "    reversed_vocab[PADDING_VALUE]=\"\"\n",
        "    reversed_vocab[UNK_VALUE]=\"UNK\"\n",
        "\n",
        "    ## YOUR CODE ENDS HERE ##\n",
        "    \n",
        "    return vocab, reversed_vocab"
      ],
      "metadata": {
        "id": "zeo9kX6i9pbH"
      },
      "id": "zeo9kX6i9pbH",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the methods you have implemented above, we can now split the dataset into training, validation, and testing sets and generate our dictionaries mapping from word tokens to IDs (and vice versa). \n",
        "\n",
        "Note: The props list currently being used splits the dataset so that 80% of samples are used to train, and the remaining 20% are evenly split between training and validation. How you split your dataset is itself a major choice and something you would need to consider in your own projects. Can you think of why?"
      ],
      "metadata": {
        "id": "w9LEk83hRFgT"
      },
      "id": "w9LEk83hRFgT"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rcmX931OFEId"
      },
      "outputs": [],
      "source": [
        "df                         = df.sample(frac=1)\n",
        "train_df, val_df, test_df  = split_train_val_test(df, props=[.8, .1, .1])\n",
        "train_vocab, reverse_vocab = generate_vocab_map(train_df)"
      ],
      "id": "rcmX931OFEId"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "CAACzA8YFEId",
        "outputId": "6785fba1-a925-49dc-dee1-71be9bdee7b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8, 0.1, 0.1)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# This line of code will help test your implementation, the expected output is the same distribution used in 'props'\n",
        "#   in the above cell. Try out some different values to ensure it works, but for submission ensure you use \n",
        "#   [.8, .1, .1] \n",
        "\n",
        "(len(train_df) / len(df)), (len(val_df) / len(df)), (len(test_df) / len(df))"
      ],
      "id": "CAACzA8YFEId"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part 1.4 Building a Dataset Class"
      ],
      "metadata": {
        "id": "5fCFfEHv1hnI"
      },
      "id": "5fCFfEHv1hnI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-qTQQa2FEIe"
      },
      "source": [
        "PyTorch has custom Dataset Classes that have very useful extentions, we want to turn our current pandas DataFrame into a subclass of Dataset so that we can iterate and sample through it for minibatch updates. **In the following cell, fill out the HeadlineDataset class.** Refer to PyTorch documentation on [Dataset Classes](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) \n",
        "for help."
      ],
      "id": "8-qTQQa2FEIe"
    },
    {
      "cell_type": "code",
      "source": [
        "# BEGIN - DO NOT CHANGE THESE IMPORTS/CONSTANTS OR IMPORT ADDITIONAL PACKAGES.\n",
        "from torch.utils.data import Dataset\n",
        "# END - DO NOT CHANGE THESE IMPORTS/CONSTANTS OR IMPORT ADDITIONAL PACKAGES.\n",
        "\n",
        "# HeadlineDataset\n",
        "# This class takes a Pandas DataFrame and wraps in a Torch Dataset.\n",
        "# Read more about Torch Datasets here: \n",
        "# https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
        "# \n",
        "class HeadlineDataset(Dataset):\n",
        "    \n",
        "    # initialize this class with appropriate instance variables\n",
        "    def __init__(self, vocab, df, max_length=50):\n",
        "        # For this method: We would *strongly* recommend storing the dataframe \n",
        "        #                  itself as an instance variable, and keeping this method\n",
        "        #                  very simple. Leave processing to __getitem__. \n",
        "        #              \n",
        "        #                  Sometimes, however, it does make sense to preprocess in \n",
        "        #                  __init__. If you are curious as to why, read the aside at the \n",
        "        #                  bottom of this cell.\n",
        "        # \n",
        "        \n",
        "        ## YOUR CODE STARTS HERE (~3 lines of code) ##\n",
        "        self.vocab=vocab\n",
        "        self.df=df\n",
        "        self.max_length=max_length\n",
        "        \n",
        "\n",
        "        return \n",
        "        ## YOUR CODE ENDS HERE ##\n",
        "    \n",
        "    # return the length of the dataframe instance variable\n",
        "    def __len__(self):\n",
        "\n",
        "        df_len = None\n",
        "        ## YOUR CODE STARTS HERE (1 line of code) ##\n",
        "        df_len=len(self.df)\n",
        "\n",
        "\n",
        "        ## YOUR CODE ENDS HERE ##\n",
        "        return df_len\n",
        "\n",
        "    # __getitem__\n",
        "    # \n",
        "    # Converts a dataframe row (row[\"tokenized\"]) to an encoded torch LongTensor,\n",
        "    # using our vocab map created using generate_vocab_map. Restricts the encoded \n",
        "    # headline length to max_length.\n",
        "    # \n",
        "    # The purpose of this method is to convert the row - a list of words - into\n",
        "    # a corresponding list of numbers.\n",
        "    #\n",
        "    # i.e. using a map of {\"hi\": 2, \"hello\": 3, \"UNK\": 0}\n",
        "    # this list [\"hi\", \"hello\", \"NOT_IN_DICT\"] will turn into [2, 3, 0]\n",
        "    #\n",
        "    # returns: \n",
        "    # tokenized_word_tensor - torch.LongTensor \n",
        "    #                         A 1D tensor of type Long, that has each\n",
        "    #                         token in the dataframe mapped to a number.\n",
        "    #                         These numbers are retrieved from the vocab_map\n",
        "    #                         we created in generate_vocab_map. \n",
        "    # \n",
        "    #                         **IMPORTANT**: if we filtered out the word \n",
        "    #                         because it's infrequent (and it doesn't exist \n",
        "    #                         in the vocab) we need to replace it w/ the UNK \n",
        "    #                         token\n",
        "    # \n",
        "    # curr_label            - int\n",
        "    #                         Binary 0/1 label retrieved from the DataFrame.\n",
        "    # \n",
        "    def __getitem__(self, index: int):\n",
        "        tokenized_word_tensor = None\n",
        "        curr_label            = None\n",
        "        ## YOUR CODE STARTS HERE (~3-7 lines of code) ##\n",
        "        curr_label=self.df[\"label\"].iloc[index]\n",
        "        result=[]\n",
        "        for i in range(len(self.df[\"tokenized\"].iloc[index])):\n",
        "          if self.df[\"tokenized\"].iloc[index][i] in self.vocab.keys() and i <self.max_length: result.append(self.vocab[self.df[\"tokenized\"].iloc[index][i]])\n",
        "          elif i <self.max_length: result.append(1)\n",
        "          else: break\n",
        "        tokenized_word_tensor=torch.LongTensor(result)\n",
        "            \n",
        "\n",
        "        \n",
        "\n",
        "        ## YOUR CODE ENDS HERE ##\n",
        "        return tokenized_word_tensor, curr_label\n",
        "\n",
        "\n",
        "\n",
        "#\n",
        "# Completely optional aside on preprocessing in __init__.\n",
        "# \n",
        "# Sometimes the compute bottleneck actually ends up being in __getitem__.\n",
        "# In this case, you'd loop over your dataset in __init__, passing data \n",
        "# to __getitem__ and storing it in another instance variable. Then,\n",
        "# you can simply return the preprocessed data in __getitem__ instead of\n",
        "# doing the preprocessing.\n",
        "# \n",
        "# There is a tradeoff though: can you think of one?\n",
        "# "
      ],
      "metadata": {
        "id": "tqt9q92J1QKK"
      },
      "id": "tqt9q92J1QKK",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "KuLtIOAZFEIe"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import RandomSampler\n",
        "\n",
        "train_dataset = HeadlineDataset(train_vocab, train_df)\n",
        "val_dataset   = HeadlineDataset(train_vocab, val_df)\n",
        "test_dataset  = HeadlineDataset(train_vocab, test_df)\n",
        "\n",
        "# Now that we're wrapping our dataframes in PyTorch datsets, we can make use of PyTorch Random Samplers, they'll\n",
        "#   define how our DataLoaders sample elements from the HeadlineDatasets  \n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "val_sampler   = RandomSampler(val_dataset)\n",
        "test_sampler  = RandomSampler(test_dataset)"
      ],
      "id": "KuLtIOAZFEIe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part 1.5 Finalizing our DataLoader"
      ],
      "metadata": {
        "id": "n9iBiSKF1yXA"
      },
      "id": "n9iBiSKF1yXA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfXSbxoFFEIe"
      },
      "source": [
        "We can now use PyTorch DataLoaders to batch our data for us. **In the following cell fill out collate_fn.** Refer to PyTorch documentation on [DataLoaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) for help."
      ],
      "id": "lfXSbxoFFEIe"
    },
    {
      "cell_type": "code",
      "source": [
        "# BEGIN - DO NOT CHANGE THESE IMPORTS/CONSTANTS OR IMPORT ADDITIONAL PACKAGES.\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "# END - DO NOT CHANGE THESE IMPORTS/CONSTANTS OR IMPORT ADDITIONAL PACKAGES.\n",
        "\n",
        "# collate_fn\n",
        "# This function is passed as a parameter to Torch DataSampler. collate_fn collects\n",
        "# batched rows, in the form of tuples, from a DataLoader and applies some final \n",
        "# pre-processing.\n",
        "#\n",
        "# Objective:\n",
        "# In our case, we need to take the batched input array of 1D tokenized_word_tensors, \n",
        "# and create a 2D tensor that's padded to be the max length from all our tokenized_word_tensors \n",
        "# in a batch. We're moving from a Python array of tuples, to a padded 2D tensor. \n",
        "#\n",
        "# *HINT*: you're allowed to use torch.nn.utils.rnn.pad_sequence (ALREADY IMPORTED)\n",
        "# \n",
        "# Finally, you can read more about collate_fn here: https://pytorch.org/docs/stable/data.html\n",
        "#\n",
        "# args: \n",
        "# batch - PythonArray[tuple(tokenized_word_tensor: 1D Torch.LongTensor, curr_label: int)]\n",
        "#         len(batch) == BATCH_SIZE\n",
        "# \n",
        "# returns:\n",
        "# padded_tokens - 2D LongTensor of shape (BATCH_SIZE, max len of all tokenized_word_tensor))\n",
        "# y_labels      - 1D FloatTensor of shape (BATCH_SIZE)\n",
        "# \n",
        "def collate_fn(batch, padding_value=PADDING_VALUE):\n",
        "    padded_tokens, y_labels = None, None\n",
        "    ## YOUR CODE STARTS HERE (~4-8 lines of code) ##\n",
        "    seq=[]\n",
        "    y=[]\n",
        "    for i in range(len(batch)):\n",
        "      seq.append(batch[i][0])\n",
        "      y.append(batch[i][1])\n",
        "    padded_tokens=torch.nn.utils.rnn.pad_sequence(seq,padding_value=PADDING_VALUE).to(torch.device('cpu'))\n",
        "    y_labels=torch.FloatTensor(y).to(torch.device('cpu'))\n",
        "    \n",
        "    \n",
        "    ## YOUR CODE ENDS HERE ##\n",
        "    return padded_tokens, y_labels"
      ],
      "metadata": {
        "id": "Zp1aQAvn1_mz"
      },
      "id": "Zp1aQAvn1_mz",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "OayoJRTeFEIf"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "train_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, collate_fn=collate_fn)\n",
        "val_iterator   = DataLoader(val_dataset, batch_size=BATCH_SIZE, sampler=val_sampler, collate_fn=collate_fn)\n",
        "test_iterator  = DataLoader(test_dataset, batch_size=BATCH_SIZE, sampler=test_sampler, collate_fn=collate_fn)"
      ],
      "id": "OayoJRTeFEIf"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "pidbg12AFEIf",
        "outputId": "36c8b620-8152-4614-91d6-22cb61a6e398",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[6448, 1086, 1839,  501,  145,  497,  830, 1581, 2558,  289, 1325, 3231,\n",
            "            1, 2671,  263, 5731],\n",
            "        [7492,    1,   11,   61,  779, 4885,   30,   37,  755,  263,  242, 7587,\n",
            "         1174, 3985,    1, 5771],\n",
            "        [  11,  200,  577, 4696,  263, 5929,   31, 3182,   81, 2868,   61,   61,\n",
            "           52,  792,   61,    1],\n",
            "        [ 630,   16,   24,   51,    1, 3250,  835,    1, 3235, 1306,  263, 1441,\n",
            "         2586,  217,  259, 3602],\n",
            "        [  19, 4475,  214,  753,  789,   52,  997,  655, 3694,  361,  238,  153,\n",
            "         4582,  405, 4287,   56],\n",
            "        [7493,   24,   52,   61,   56,  306, 2041, 1849, 3170,   30,   15, 1287,\n",
            "         1801,  952,   44,   33],\n",
            "        [   0, 1232,    1,  865,   15,  153,  640,   52,   30,  228, 7368,   52,\n",
            "          817,   81, 6522,    1],\n",
            "        [   0,   52, 2487, 6854, 3827,  848,   33, 6042,   76, 2284,  570,  598,\n",
            "          339, 1999,   52,  158],\n",
            "        [   0,  306, 1866,  113, 2973, 3929,   81, 1844, 4052,  153,   34,  297,\n",
            "           81,   36, 1644, 1072],\n",
            "        [   0, 1297,   12,    1, 5224,  217, 3958, 7116, 7588,   61, 3041,   81,\n",
            "         6029,  145,    1,   19],\n",
            "        [   0, 5721,  987, 1359, 1079,  263, 3959,    0,   52, 1909, 3042, 7563,\n",
            "           30,  276,   15,   78],\n",
            "        [   0,    0,    1,   52,  263,  479,   30,    0,  203,  579,   12, 4768,\n",
            "          263,   30, 6523, 6826],\n",
            "        [   0,    0,    0,  801,  479,    0,    0,    0, 5901, 3162,  128,    1,\n",
            "         2265,  228, 4530,    0],\n",
            "        [   0,    0,    0,   52, 1337,    0,    0,    0,  913,   36, 2823,   30,\n",
            "            0,  306,   33,    0],\n",
            "        [   0,    0,    0, 1657,  489,    0,    0,    0,   33, 6193,  221,    0,\n",
            "            0,   36,  321,    0],\n",
            "        [   0,    0,    0,  713,  158,    0,    0,    0,  327, 6194,   52,    0,\n",
            "            0,   51,  417,    0],\n",
            "        [   0,    0,    0, 7473, 1662,    0,    0,    0,   52, 2889, 5161,    0,\n",
            "            0,   28,  255,    0],\n",
            "        [   0,    0,    0,  297,  317,    0,    0,    0, 5813, 1675,  259,    0,\n",
            "            0, 4062,    0,    0],\n",
            "        [   0,    0,    0, 1657,  962,    0,    0,    0,  593,    1, 7369,    0,\n",
            "            0,  783,    0,    0],\n",
            "        [   0,    0,    0,    1,    0,    0,    0,    0, 3622,  437, 6429,    0,\n",
            "            0,  657,    0,    0],\n",
            "        [   0,    0,    0,   36,    0,    0,    0,    0,    0,  263,   21,    0,\n",
            "            0,   30,    0,    0],\n",
            "        [   0,    0,    0,    1,    0,    0,    0,    0,    0,  633,  469,    0,\n",
            "            0,    0,    0,    0],\n",
            "        [   0,    0,    0,  354,    0,    0,    0,    0,    0, 3618, 7368,    0,\n",
            "            0,    0,    0,    0],\n",
            "        [   0,    0,    0, 1772,    0,    0,    0,    0,    0, 3067, 5132,    0,\n",
            "            0,    0,    0,    0],\n",
            "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,   36,    0,    0,\n",
            "            0,    0,    0,    0],\n",
            "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,  431,    0,    0,\n",
            "            0,    0,    0,    0],\n",
            "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,  927,    0,    0,\n",
            "            0,    0,    0,    0],\n",
            "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,   15,    0,    0,\n",
            "            0,    0,    0,    0],\n",
            "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    1,    0,    0,\n",
            "            0,    0,    0,    0],\n",
            "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,  517,    0,    0,\n",
            "            0,    0,    0,    0]]) tensor([0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0.])\n",
            "x: torch.Size([30, 16])\n",
            "y: torch.Size([16])\n"
          ]
        }
      ],
      "source": [
        "# Use this to test your collate_fn implementation.\n",
        "# You can look at the shapes of x and y or put print statements in collate_fn while running this snippet\n",
        "\n",
        "for x, y in test_iterator:\n",
        "  print(x, y)\n",
        "  print(f'x: {x.shape}')\n",
        "  print(f'y: {y.shape}')\n",
        "  break\n",
        "test_iterator = DataLoader(test_dataset, batch_size=BATCH_SIZE, sampler=test_sampler, collate_fn=collate_fn)"
      ],
      "id": "pidbg12AFEIf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWLK7T1uFEIg"
      },
      "source": [
        "### Part 2: Modeling [10 pts]\n",
        "Let's move to modeling, now that we have dataset iterators that batch our data for us. In the following code block, you'll build a feed-forward neural network implementing a neural bag-of-words baseline, NBOW-RAND, described in section 2.1 of [this paper](https://www.aclweb.org/anthology/P15-1162.pdf). You'll find [this](https://pytorch.org/docs/stable/nn.html) page useful for understanding the different layers and [this](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html) page useful for how to put them into action.\n",
        "\n",
        "The core idea behind this baseline is that after we embed each word for a document, we average the embeddings to produce a single vector that hopefully captures some general information spread across the sequence of embeddings. This means we first turn each document of length *n* into a matrix of *nxd*, where *d* is the dimension of the embedding. Then we average this matrix to produce a vector of length *d*, summarizing the contents of the document and proceed with the rest of the network. \n",
        "\n",
        "While you're working through this implementation, keep in mind how the dimensions change and what each axes represents, as documents will be passed in as minibatches requiring careful selection of which axes you apply certain operations too. You're more than welcome to experiment with the architecture of this network as well outside of the basic setup we describe below, such as adding in other layers, to see how this changes your results."
      ],
      "id": "BWLK7T1uFEIg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part 2.1 Define the NBOW model class"
      ],
      "metadata": {
        "id": "pZDPs0Sf-H3V"
      },
      "id": "pZDPs0Sf-H3V"
    },
    {
      "cell_type": "code",
      "source": [
        "# BEGIN - DO NOT CHANGE THESE IMPORTS OR IMPORT ADDITIONAL PACKAGES.\n",
        "import torch.nn as nn\n",
        "# END - DO NOT CHANGE THESE IMPORTS OR IMPORT ADDITIONAL PACKAGES.\n",
        "\n",
        "class NBOW(nn.Module):\n",
        "    # Instantiate layers for your model-\n",
        "    # \n",
        "    # Your model architecture will be a feed-forward neural network.\n",
        "    #\n",
        "    # You'll need 3 nn.Modules at minimum\n",
        "    # 1. An embeddings layer (see nn.Embedding)\n",
        "    # 2. A linear layer (see nn.Linear)\n",
        "    # 3. A sigmoid output (see nn.Sigmoid)\n",
        "    #\n",
        "    # HINT: In the forward step, the BATCH_SIZE is the first dimension.\n",
        "    # \n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        ## YOUR CODE STARTS HERE (~4 lines of code) ##\n",
        "        self.embedding=nn.Embedding(vocab_size,embedding_dim)\n",
        "        self.linear=nn.Linear(embedding_dim,1)\n",
        "        self.sigmoid=nn.Sigmoid()\n",
        "        \n",
        "\n",
        "        ## YOUR CODE ENDS HERE ##\n",
        "        \n",
        "    # Complete the forward pass of the model.\n",
        "    #\n",
        "    # Use the output of the embedding layer to create\n",
        "    # the average vector, which will be input into the \n",
        "    # linear layer.\n",
        "    # \n",
        "    # args:\n",
        "    # x - 2D LongTensor of shape (BATCH_SIZE, max len of all tokenized_word_tensor))\n",
        "    #     This is the same output that comes out of the collate_fn function you completed\n",
        "    def forward(self, x):\n",
        "        ## YOUR CODE STARTS HERE (~4-5 lines of code) ##\n",
        "        output=self.embedding(x)\n",
        "        output=output.mean(-3)\n",
        "        x=self.linear(output)\n",
        "        x=self.sigmoid(x)\n",
        "\n",
        "\n",
        "        return x\n",
        "        ## YOUR CODE ENDS HERE ##\n",
        "    "
      ],
      "metadata": {
        "id": "jzGx2q0jLqyU"
      },
      "execution_count": 15,
      "outputs": [],
      "id": "jzGx2q0jLqyU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part 2.2 Initialize the NBOW classification model\n",
        "\n",
        "Since the NBOW model is rather basic, assuming you haven't added any additional layers, there's really only one hyperparameter for the model architecture: the size of the embedding dimension. \n",
        "\n",
        "The vocab_size parameter here is based on the number of unique words kept in the vocab after removing those occurring too infrequently, so this is determined by our dataset and is in turn not a true hyperparameter (though the cutoff we used previously might be). The embedding_dim parameter dictates what size vector each word can be embedded as. \n",
        "\n",
        "If you added additional linear layers to the NBOW model then the input/output dimensions of each would be considered a hyperparameter you might want to experiment with. While the sizes are constrained based on previous & following layers (the number of dimensions need to match for the matrix multiplication), whatever sequence you used could still be tweaked in various ways. \n",
        "\n",
        "A special note concerning the model initialization: We're specifically sending the model to the device set in Part 1, to speed up training if the GPU is available. **Be aware**, you'll have to ensure other tensors are on the same device inside your training and validation loops. "
      ],
      "metadata": {
        "id": "xltosIzM-SP2"
      },
      "id": "xltosIzM-SP2"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "_HQWUu-ZFEIg"
      },
      "outputs": [],
      "source": [
        "model = NBOW(vocab_size    = len(train_vocab.keys()),\n",
        "             embedding_dim = 300).to(torch.device('cpu'))"
      ],
      "id": "_HQWUu-ZFEIg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part 2.3 Instantiate the loss function and optimizer"
      ],
      "metadata": {
        "id": "C4CZnj1f-da-"
      },
      "id": "C4CZnj1f-da-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aXi8nA0FEIh"
      },
      "source": [
        "In the following cell, **select and instantiate an appropriate loss function and optimizer.** \n",
        "\n",
        "Hint: we already use sigmoid in our model. What loss functions are availible for binary classification? Feel free to look at [PyTorch docs](https://pytorch.org/docs/stable/nn.html#loss-functions) for help!"
      ],
      "id": "9aXi8nA0FEIh"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "w98UvlXxFEIh"
      },
      "outputs": [],
      "source": [
        "#while Adam is already imported, you can try other optimizers as well\n",
        "from torch.optim import Adam\n",
        "\n",
        "criterion, optimizer = None, None\n",
        "### YOUR CODE GOES HERE ###\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "optimizer=Adam(model.parameters())\n",
        "\n",
        "\n",
        "### YOUR CODE ENDS HERE ###"
      ],
      "id": "w98UvlXxFEIh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, we have a NBOW model to classify headlines as being real or fake and a loss function/optimizer to train the model using the training dataset."
      ],
      "metadata": {
        "id": "hUXBtqPEjiRe"
      },
      "id": "hUXBtqPEjiRe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVLeTa8wFEIh"
      },
      "source": [
        "### Part 3: Training and Evaluation [10 Points]\n",
        "The final part of this HW involves training the model, and evaluating it at each epoch. **Fill out the train and test loops below. Treat real headlines as False, and Onion headlines as True.**  Feel free to look at [PyTorch docs](https://pytorch.org/tutorials/beginner/introyt/trainingyt.html) for help!"
      ],
      "id": "bVLeTa8wFEIh"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "vganx5fCFEIh"
      },
      "outputs": [],
      "source": [
        "# returns the total loss calculated from criterion\n",
        "def train_loop(model, criterion, optim, iterator):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for x, y in tqdm(iterator):\n",
        "        ### YOUR CODE STARTS HERE (~6 lines of code) ###\n",
        "        optim.zero_grad()\n",
        "        x[x>=len(train_vocab.keys())]=1\n",
        "        output=model(x)\n",
        "        #output2=torch.ones(len(output))\n",
        "        #output2[output[:,0]-output[:,1]<0]=0\n",
        "        output=criterion(output.flatten(),y)\n",
        "        total_loss+=output.item()\n",
        "        output.backward()\n",
        "        optim.step() \n",
        "        \n",
        "\n",
        "        ### YOUR CODE ENDS HERE ###\n",
        "    return total_loss\n",
        "\n",
        "# returns:\n",
        "# - true: a Python boolean array of all the ground truth values \n",
        "#         taken from the dataset iterator\n",
        "# - pred: a Python boolean array of all model predictions. \n",
        "def val_loop(model, iterator):\n",
        "    true, pred = [], []\n",
        "    ### YOUR CODE STARTS HERE (~8 lines of code) ###\n",
        "    model.eval()\n",
        "    for x, y in tqdm(iterator):\n",
        "      x[x>=len(train_vocab.keys())]=1\n",
        "      true.append(y>0)\n",
        "      pred.append((model(x)>=0.5).flatten())\n",
        "\n",
        "      \n",
        "    \n",
        "\n",
        "    ### YOUR CODE ENDS HERE ###\n",
        "    return true, pred"
      ],
      "id": "vganx5fCFEIh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part 3.1 Define the evaluation metrics"
      ],
      "metadata": {
        "id": "JNXJevTu-tDZ"
      },
      "id": "JNXJevTu-tDZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IsZQs3rFEIi"
      },
      "source": [
        "We also need evaluation metrics that tell us how well our model is doing on the validation set at each epoch and later how well the model does on the held-out test set. **Complete the functions in the following cell.** You'll find subsection 4.4.1 of Eisenstein useful for this task."
      ],
      "id": "7IsZQs3rFEIi"
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT IMPORT ANYTHING IN THIS CELL. You shouldn't need any external libraries.\n",
        "\n",
        "# accuracy\n",
        "#\n",
        "# What percent of classifications are correct?\n",
        "# \n",
        "# true: ground truth, Python list of booleans.\n",
        "# pred: model predictions, Python list of booleans.\n",
        "# return: percent accuracy bounded between [0, 1]\n",
        "#\n",
        "def accuracy(true, pred):\n",
        "    acc = None\n",
        "    ## YOUR CODE STARTS HERE (~2-5 lines of code) ##\n",
        "    result=0\n",
        "    for i in range(len(true)):\n",
        "      result+=sum([1 if true[i][j]==pred[i][j] else 0 for j in range(len(true[i]))])\n",
        "    acc=result/(len(true)*len(true[0]))\n",
        "    \n",
        "\n",
        "    ## YOUR CODE ENDS HERE ##\n",
        "    return acc\n",
        "\n",
        "# binary_f1 \n",
        "#\n",
        "# A method to calculate F-1 scores for a binary classification task.\n",
        "# \n",
        "# args -\n",
        "# true: ground truth, Python list of booleans.\n",
        "# pred: model predictions, Python list of booleans.\n",
        "# selected_class: Boolean - the selected class the F-1 \n",
        "#                 is being calculated for.\n",
        "# \n",
        "# return: F-1 score between [0, 1]\n",
        "#\n",
        "def binary_f1(true, pred, selected_class=True):\n",
        "    f1 = None\n",
        "    ## YOUR CODE STARTS HERE (~10-15 lines of code) ##\n",
        "    tp,fp,fn=0,0,0\n",
        "    for i in range(len(true)):\n",
        "      tp+=sum([1 if true[i][j]==selected_class and pred[i][j]==selected_class else 0 for j in range(len(true[i]))])\n",
        "      fp+=sum([1 if true[i][j]!=selected_class and pred[i][j]==selected_class else 0 for j in range(len(true[i]))])\n",
        "      fn+=sum([1 if true[i][j]==selected_class and pred[i][j]!=selected_class else 0 for j in range(len(true[i]))])\n",
        "    precise,recall,f1=0,0,0\n",
        "    if tp+fp!=0:\n",
        "      precise=tp/(tp+fp)\n",
        "    if tp+fn!=0:\n",
        "      recall=tp/(tp+fn)\n",
        "    if precise+recall!=0:\n",
        "      f1=2*precise*recall/(precise+recall)\n",
        "    ## YOUR CODE ENDS HERE ##\n",
        "    return f1\n",
        "\n",
        "# binary_macro_f1\n",
        "# \n",
        "# Averaged F-1 for all selected (true/false) classes.\n",
        "#\n",
        "# args -\n",
        "# true: ground truth, Python list of booleans.\n",
        "# pred: model predictions, Python list of booleans.\n",
        "#\n",
        "#\n",
        "def binary_macro_f1(true, pred):\n",
        "    averaged_macro_f1 = None\n",
        "    ## YOUR CODE STARTS HERE (1 line of code) ##\n",
        "    averaged_macro_f1=(binary_f1(true, pred, selected_class=True)+binary_f1(true, pred, selected_class=False))/2\n",
        "\n",
        "\n",
        "    ## YOUR CODE ENDS HERE ##\n",
        "    return averaged_macro_f1"
      ],
      "metadata": {
        "id": "gMQDg9Vy-wY0"
      },
      "id": "gMQDg9Vy-wY0",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Yw79JFieFEIi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "5289d9f6-ed06-49ce-fa50-96e34f9816f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 150/150 [00:00<00:00, 227.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Binary Macro F1: 0.47067245657958223\n",
            "Accuracy: 0.47375\n"
          ]
        }
      ],
      "source": [
        "# To test your eval implementation, let's see how well the untrained model does on our dev dataset.\n",
        "# It should do pretty poorly, but this can be random because of the initialization of the parameters of the model.\n",
        "true, pred = val_loop(model, val_iterator)\n",
        "print()\n",
        "print(f'Binary Macro F1: {binary_macro_f1(true, pred)}')\n",
        "print(f'Accuracy: {accuracy(true, pred)}')"
      ],
      "id": "Yw79JFieFEIi"
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, we have our datasets defined and split, our model and training tools/loops, and evaluation metrics so we can finally move on to train our model and see how it does!"
      ],
      "metadata": {
        "id": "BerBx-T3kZtC"
      },
      "id": "BerBx-T3kZtC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2to0kWVFEIi"
      },
      "source": [
        "### Part 4: Actually training the model [1 point]\n",
        "Watch your model train :D You should be able to achieve a validation F-1 score of at least .8 if everything went correctly. **Feel free to adjust the number of epochs to prevent overfitting or underfitting and to play with your model hyperparameters/optimizer & loss function.**"
      ],
      "id": "Q2to0kWVFEIi"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "N-iuqkKCFEIj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "7db6858e-23b1-4c7e-d693-97a0ce371566"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:23<00:00, 52.01it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 231.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 0\n",
            "TRAIN LOSS: 18890.277282714844\n",
            "VAL F-1: 0.7993518489385516\n",
            "VAL ACC: 0.80625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:22<00:00, 52.77it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 233.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 1\n",
            "TRAIN LOSS: 17867.287763357162\n",
            "VAL F-1: 0.8405264530773069\n",
            "VAL ACC: 0.84875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:22<00:00, 53.19it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 228.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 2\n",
            "TRAIN LOSS: 17513.622549295425\n",
            "VAL F-1: 0.8512985441090016\n",
            "VAL ACC: 0.8566666666666667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:22<00:00, 52.68it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 226.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 3\n",
            "TRAIN LOSS: 17344.50214624405\n",
            "VAL F-1: 0.8529980657640233\n",
            "VAL ACC: 0.8575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:22<00:00, 53.10it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 227.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 4\n",
            "TRAIN LOSS: 17198.194375872612\n",
            "VAL F-1: 0.8645125911259014\n",
            "VAL ACC: 0.8704166666666666\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:22<00:00, 52.48it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 231.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 5\n",
            "TRAIN LOSS: 17105.03038573265\n",
            "VAL F-1: 0.8593530239099859\n",
            "VAL ACC: 0.8641666666666666\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:23<00:00, 52.04it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 229.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 6\n",
            "TRAIN LOSS: 17019.82031095028\n",
            "VAL F-1: 0.8583881985834851\n",
            "VAL ACC: 0.86375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:23<00:00, 52.03it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 230.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 7\n",
            "TRAIN LOSS: 16952.57702934742\n",
            "VAL F-1: 0.8609257989364165\n",
            "VAL ACC: 0.8670833333333333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:22<00:00, 52.33it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 225.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 8\n",
            "TRAIN LOSS: 16906.418324828148\n",
            "VAL F-1: 0.859436114732725\n",
            "VAL ACC: 0.8658333333333333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:23<00:00, 51.78it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 230.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 9\n",
            "TRAIN LOSS: 16854.88186597824\n",
            "VAL F-1: 0.8582439335068677\n",
            "VAL ACC: 0.86375\n"
          ]
        }
      ],
      "source": [
        "TOTAL_EPOCHS = 10\n",
        "for epoch in range(TOTAL_EPOCHS):\n",
        "    train_loss = train_loop(model, criterion, optimizer, train_iterator)\n",
        "    true, pred = val_loop(model, val_iterator)\n",
        "    print(f\"EPOCH: {epoch}\")\n",
        "    print(f\"TRAIN LOSS: {train_loss}\")\n",
        "    print(f\"VAL F-1: {binary_macro_f1(true, pred)}\")\n",
        "    print(f\"VAL ACC: {accuracy(true, pred)}\")"
      ],
      "id": "N-iuqkKCFEIj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_l91F4ooFEIj"
      },
      "source": [
        "We can also look at the models performance on the held-out test set, using the same val_loop we wrote earlier."
      ],
      "id": "_l91F4ooFEIj"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "vs8Fy_ncFEIo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "75011190-8f65-4060-dafe-31a3a5756c14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 150/150 [00:00<00:00, 225.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TEST F-1: 0.9630030732336707\n",
            "TEST ACC: 0.9654166666666667\n"
          ]
        }
      ],
      "source": [
        "true, pred = val_loop(model, test_iterator)\n",
        "print()\n",
        "print(f\"TEST F-1: {binary_macro_f1(true, pred)}\")\n",
        "print(f\"TEST ACC: {accuracy(true, pred)}\")"
      ],
      "id": "vs8Fy_ncFEIo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMPWmorEFEIp"
      },
      "source": [
        "### Part 5: Analysis [5 points]\n",
        "Answer the following questions:\n",
        "\n"
      ],
      "id": "rMPWmorEFEIp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. What happens to the vocab size as you change the cutoff in the cell below? Can you explain this in the context of [Zipf's Law](https://en.wikipedia.org/wiki/Zipf%27s_law)?\n",
        "\n",
        "Answer: The size of vocab will extend about 1/3 (about 9500 to about 13000). According to Zipf's law: *stating that given some corpus of natural language utterances, the frequency of any word is inversely proportional to its rank in the frequency table.* Therefore, when cutoff is lower to 1, the words appears twice will be counted. The number of these words is about 1/3 of the total number of the words in higher rank."
      ],
      "metadata": {
        "id": "fnjKlKt352hQ"
      },
      "id": "fnjKlKt352hQ"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "pI0fM4oMFEIp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "c0b70575-5c69-4783-98e9-ea13781b9afb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13298"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "tmp_vocab, _ = generate_vocab_map(train_df, cutoff = 1)\n",
        "len(tmp_vocab)"
      ],
      "id": "pI0fM4oMFEIp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0x54B1lFEIp"
      },
      "source": [
        "#### 2. Can you describe what cases the model is getting wrong in the witheld test-set? \n",
        "\n",
        "Answer: According to the sentences printed, I find that a large proportion of the sentences have \"UNK\" or \"\", the unknown words and padding words, which will interference the model's judgement."
      ],
      "id": "d0x54B1lFEIp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "To do this, you'll need to create a new val_train_loop (``val_train_loop_incorrect``) so it returns incorrect sequences **and** you'll need to decode these sequences back into words. \n",
        "Thankfully, you've already created a map that can convert encoded sequences back to regular English: you will find the ``reverse_vocab`` variable useful.\n",
        "\n",
        "```\n",
        "# i.e. using a reversed map of {\"hi\": 2, \"hello\": 3, \"UNK\": 0}\n",
        "# we can turn [2, 3, 0] into this => [\"hi\", \"hello\", \"UNK\"]\n",
        "```"
      ],
      "metadata": {
        "id": "_yiIZov-583w"
      },
      "id": "_yiIZov-583w"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "TfohtPF8FEIp"
      },
      "outputs": [],
      "source": [
        "# Implement this however you like! It should look very similar to val_loop.\n",
        "# Pass the test_iterator through this function to look at errors in the test set.\n",
        "def val_train_loop_incorrect(model, iterator):\n",
        "    model.eval()\n",
        "    for x, y in tqdm(iterator):\n",
        "      x[x>=len(train_vocab.keys())]=1\n",
        "      true=(y>0)\n",
        "      pred=(model(x)>=0.5).flatten()\n",
        "      for i in range(len(true)):\n",
        "        if pred[i]!=true[i]:\n",
        "          print()\n",
        "          for j in range(len(x[i])):\n",
        "            print(reverse_vocab[x[i][j].item()],end=\" \")\n",
        "          print()\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "   "
      ],
      "id": "TfohtPF8FEIp"
    },
    {
      "cell_type": "code",
      "source": [
        "val_train_loop_incorrect(model, test_iterator)"
      ],
      "metadata": {
        "id": "6-azPje88iU0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "8768054f-b0d4-4d61-abdc-a68fa048e328"
      },
      "id": "6-azPje88iU0",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  8%|▊         | 12/150 [00:00<00:01, 117.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " gay    if source    times   for   \n",
            "\n",
            "humanity neil harvard young study UNK in life news peter mark for lebron grandmother the zach \n",
            "\n",
            "' laptop through jesus challenge UNK holds be , on to out birthday perspective gary the \n",
            "\n",
            "25-year-old kids nation once who cartel hampshire you `` guards UNK social many charged village 's \n",
            "\n",
            "UNK  '  neighbors  is   by  algorithm increasing of  kitchen \n",
            "\n",
            "n't on bike hot belief , [ public  of to  water to UNK for \n",
            "\n",
            "’ take to the steel  UNK   battle  game  chernobyl as an \n",
            "\n",
            "of initials meal UNK are in UNK acts for find of journalist t up legislators bad \n",
            "\n",
            "     event     punish     privacy \n",
            "\n",
            "state citizen of ' 38 in president bottom UNK after dog with rental of skull armed \n",
            "\n",
            "contest the still  didn  ,    just      \n",
            "\n",
            "so human a goat close is UNK  student UNK chicken , after after ” state \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 24%|██▍       | 36/150 [00:00<00:01, 106.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "are UNK 's attacks UNK shutdown assignment  that kim flight portraying woman porn orgies family \n",
            "\n",
            "die call ' in treatment if to  has refusing russian outraged everyday because and to \n",
            "\n",
            "of , female manufacturing , pole  powers to id UNK due honored talking ' dumbass \n",
            "\n",
            "storage  six day UNK to offering but said inside considered applicants women says  raw \n",
            "\n",
            "man of detained with he date corruption denied are his stone canadian made : year many \n",
            "\n",
            "little praised hours instagram change she UNK despite ’ video parents UNK bricks as found things \n",
            "\n",
            "the for at miles to and ’  a them  the  a indian  \n",
            "\n",
            "claims carbon asks blocks press boy : says cat of : finds builds UNK ' getting \n",
            "\n",
            "would bringing happens bbc : out no is saved his UNK back trench UNK too hand \n",
            "\n",
            "hillary mother elizabeth : cop to over solve in / posts , , will students this \n",
            "\n",
            "the on  could   takes      is reports  that \n",
            "\n",
            "UNK surprised ’ to finds use body arrest city arrested military alex to not whoa house \n",
            "\n",
            "entering in words a the  the liked beating $ guard a  way t-shirt him \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 38%|███▊      | 57/150 [00:00<00:00, 95.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "since   dad  '     someone review     \n",
            "\n",
            "is UNK : paper up n't this leaked stolen have old for `` ensure after no \n",
            "\n",
            "lizard on have to before  ’ urging nipples 30 it  meeting hide  says \n",
            "\n",
            "green  ali ' ’ don were of on 's chief poor  heard packs return \n",
            "\n",
            "speech   vegan , their course  pays      cheddar toilet \n",
            "\n",
            "accused lizard back claims finds blames ‘ UNK remind imagine UNK copycat stations $ files nothing \n",
            "\n",
            "of : in that staring legal venus since hillary how workers ' urge UNK for like \n",
            "\n",
            "is world to 're using court at with the  many his pete beheadings onion carve \n",
            "\n",
            "’ sex  a  it backpack of  UNK   u.s. drunk challenged by \n",
            "\n",
            "  with who does  search  son  the gun   in  \n",
            "\n",
            "human strip  UNK review to in clickhole memoir i   chips UNK for  \n",
            "\n",
            "man theonion toddler $ most UNK brussels internet porn new indiana france happy barbra man canadian \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 54%|█████▍    | 81/150 [00:00<00:00, 108.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ", have ’ , history against man bowl photos 11-year-old $ UNK : trump by for \n",
            "\n",
            "point  santa wide UNK  for   laser internet  UNK    \n",
            "\n",
            "first |     is   court    those  in \n",
            "\n",
            "woman disabled men iphone UNK downing chris florida $ car indian police raccoon is louisville russia \n",
            "\n",
            "in paid of , UNK i 11 that  pay influence up just  to stage \n",
            "\n",
            "to the  ,   fight  semen control   gay supports they  \n",
            "\n",
            "in  dead bill UNK ,   out  passes eating  the UNK the \n",
            "\n",
            "  inspired    for   a  says at  mom  \n",
            "\n",
            "song UNK 'the student amid get after , a they four-year-old s organising volunteers commission is \n",
            "\n",
            " is news kids  marijuana  hotel  americans mars   a it  \n",
            "\n",
            " city     zone   with     the  \n",
            "\n",
            " rat     ’   man     course  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 72%|███████▏  | 108/150 [00:00<00:00, 119.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "injured his pilots his UNK against refugees UNK name  help place make  a UNK \n",
            "\n",
            "to to about his full pack freezer minute get challenge 's not UNK district from , \n",
            "\n",
            "nightmare 5 arrested UNK face bud do ’ UNK ? psychic doesn syria  vs. for \n",
            "\n",
            " long  UNK     UNK    UNK    \n",
            "\n",
            "three instagram boat even ways years her that in his coming death make grill hid to \n",
            "\n",
            "  psa    sperm  a     landed when  \n",
            "\n",
            " in redskins  is    australia culture electric  ’  mountain  \n",
            "\n",
            "kicking  in birthday goodie win save nation iran ‘ it threatens UNK shut 's might \n",
            "\n",
            "majority services students : mobile sure ’ tells partly takes heartbreaking bans $ spike experiments college \n",
            "\n",
            "mom sneak on children from ' more not blows ’ from force both need it in \n",
            "\n",
            "     internet `` say         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 89%|████████▊ | 133/150 [00:01<00:00, 108.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "is fluid  climate shame   before capable round   ..... open  UNK \n",
            "\n",
            "of losing flat UNK through UNK after sandler spot doing to syria 'to 5  3,000 \n",
            "\n",
            "             ,  in \n",
            "\n",
            "master is as come bailout probation  on UNK assault son judge dropping to of site \n",
            "\n",
            "his hasn UNK minnesota a arrested  pornography building  in  a mountain from of \n",
            "\n",
            "heartless if in ’ biden get he s ' abused jesus running friend 120  just \n",
            "\n",
            " business  within  clothing movie for access UNK bring UNK i  pictures ’ \n",
            "\n",
            ":  he shelter being have to , no you man opened when porn in speaking \n",
            "\n",
            "guy   them  me conditions says -   .     \n",
            "\n",
            "twerk arrested for UNK facebook would a UNK in to can luggage UNK for centre 'd \n",
            "\n",
            "   telling sixth   cops   trump and     \n",
            "\n",
            "voter new it a war you half-marathon 's with a washer into passes sad mystery least \n",
            "\n",
            "with    asking trafficking he her      yard  ’ \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 150/150 [00:01<00:00, 107.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  your  a     'no  in were ,  in \n",
            "\n",
            "‘     ,  t    ice UNK    \n",
            "\n",
            "racist  i a been | that because lose , `` UNK rival ’ to then \n",
            "\n",
            "someone for creation tattoos u.s. too single say violent it only restaurant rat new failed diaper \n",
            "\n",
            "smoking  i  a  menu from   ’ ’     \n",
            "\n",
            " resign to into '  not  stone perth   is UNK  elected \n",
            "\n",
            "trump ’ minister can UNK wearing diego bridge searching man warn at piece accused players working \n",
            "\n",
            "him t crying ones  the years 50 time fish  life company of agents could \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 6: LSTM Model [Extra-Credit, 4 points]"
      ],
      "metadata": {
        "id": "Ie9VqRbg78Ty"
      },
      "id": "Ie9VqRbg78Ty"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part 6.1 Define the RecurrentModel class\n",
        "Something that has been overlooked so far in this project is the sequential structure to language: a word typically only has a clear meaning because of its relationship to the words before and after it in the sequence, and the feed-forward network of Part 2 cannot model this type of data. A solution to this, is the use of [recurrent neural networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/). These types of networks not only produce some output given some step from a sequence, but also update their internal state, hopefully \"remembering\" some information about the previous steps in the input sequence. Of course, they do have their own faults, but we'll cover this more thoroughly later in the semester. \n",
        "\n",
        "Your task for the extra credit portion of this assignment, is to implement such a model below using a LSTM. Instead of averaging the embeddings as with the FFN in Part 2, you'll instead feed all of these embeddings to a LSTM layer, get its final output, and use this to make your prediction for the class of the headline. "
      ],
      "metadata": {
        "id": "gXWSfPfBA4XU"
      },
      "id": "gXWSfPfBA4XU"
    },
    {
      "cell_type": "code",
      "source": [
        "class RecurrentModel(nn.Module):\n",
        "    # Instantiate layers for your model-\n",
        "    # \n",
        "    # Your model architecture will be an optionally bidirectional LSTM,\n",
        "    # followed by a linear + sigmoid layer.\n",
        "    #\n",
        "    # You'll need 4 nn.Modules\n",
        "    # 1. An embeddings layer (see nn.Embedding)\n",
        "    # 2. A bidirectional LSTM (see nn.LSTM)\n",
        "    # 3. A Linear layer (see nn.Linear)\n",
        "    # 4. A sigmoid output (see nn.Sigmoid)\n",
        "    #\n",
        "    # HINT: In the forward step, the BATCH_SIZE is the first dimension.\n",
        "    # HINT: Think about what happens to the linear layer's hidden_dim size\n",
        "    #       if bidirectional is True or False.\n",
        "    # \n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, \\\n",
        "                 num_layers=1, bidirectional=True):\n",
        "        super().__init__()\n",
        "        ## YOUR CODE STARTS HERE (~4 lines of code) ##\n",
        "        self.embedding=nn.Embedding(vocab_size,embedding_dim)\n",
        "        self.lstm=nn.LSTM(input_size=embedding_dim,hidden_size=hidden_dim,num_layers=num_layers,bidirectional=bidirectional)\n",
        "        self.linear=nn.Linear(hidden_dim,1)\n",
        "        self.sigmoid=nn.Sigmoid()\n",
        "        \n",
        "\n",
        "        ## YOUR CODE ENDS HERE ##\n",
        "        \n",
        "    # Complete the forward pass of the model.\n",
        "    #\n",
        "    # Use the last timestep of the output of the LSTM as input\n",
        "    # to the linear layer. This will only require some indexing \n",
        "    # into the correct return from the LSTM layer. \n",
        "    # \n",
        "    # args:\n",
        "    # x - 2D LongTensor of shape (BATCH_SIZE, max len of all tokenized_word_tensor))\n",
        "    #     This is the same output that comes out of the collate_fn function you completed-\n",
        "    def forward(self, x):\n",
        "        ## YOUR CODE STARTS HERE (~4-5 lines of code) ##\n",
        "        embeds=self.embedding(x)\n",
        "        embeds=embeds.mean(-3)\n",
        "        lstm_out, _ = self.lstm(embeds)\n",
        "        linear_output=self.linear(lstm_out[:,300:])#self.linear(lstm_out.view(len(x), len(lstm_out)))\n",
        "        x=self.sigmoid(linear_output)\n",
        "\n",
        "        return x\n",
        "        ## YOUR CODE ENDS HERE ##\n",
        "    "
      ],
      "metadata": {
        "id": "YN8zvhLJ-MVJ"
      },
      "id": "YN8zvhLJ-MVJ",
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the RecurrentModel is defined, we'll reinitialize our dataset iterators so they're back at the start. "
      ],
      "metadata": {
        "id": "HprkOm-fAVyj"
      },
      "id": "HprkOm-fAVyj"
    },
    {
      "cell_type": "code",
      "source": [
        "train_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, collate_fn=collate_fn)\n",
        "val_iterator   = DataLoader(val_dataset, batch_size=BATCH_SIZE, sampler=val_sampler, collate_fn=collate_fn)\n",
        "test_iterator  = DataLoader(test_dataset, batch_size=BATCH_SIZE, sampler=test_sampler, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "6-uftfXEAqOi"
      },
      "id": "6-uftfXEAqOi",
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part 6.2 Initialize the LSTM classification model\n",
        "\n",
        "Next we need to initialize our new model, as well as define it's optimizer and loss function as we did for the FFN. Feel free to use the same optimizer you did above, or see how this model reacts to different optimizers/learning rates than the FFN.  "
      ],
      "metadata": {
        "id": "2qROtRw3AtZy"
      },
      "id": "2qROtRw3AtZy"
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_model = RecurrentModel(vocab_size    = len(train_vocab.keys()),\n",
        "                            embedding_dim = 300,\n",
        "                            hidden_dim    = 300,\n",
        "                            num_layers    = 1,\n",
        "                            bidirectional = True).to(torch.device('cpu'))"
      ],
      "metadata": {
        "id": "LNWcLJpsBRzg"
      },
      "id": "LNWcLJpsBRzg",
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_criterion, lstm_optimizer = None, None\n",
        "### YOUR CODE STARTS HERE ###\n",
        "from torch.optim import Adam\n",
        "lstm_criterion=nn.CrossEntropyLoss()\n",
        "lstm_optimizer=Adam(lstm_model.parameters())\n",
        "\n",
        "\n",
        "### YOUR CODE ENDS HERE ###"
      ],
      "metadata": {
        "id": "OdTxe0bFBqnP"
      },
      "id": "OdTxe0bFBqnP",
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part 6.3 Training and Evaluation\n",
        "\n",
        "Because the only difference between this model and the FFN is the internal structure, we can use the same methods as above to evaluate and train it. You should be able to achieve a validation F-1 score of at least .8 if everything went correctly. **Feel free to adjust the number of epochs to prevent overfitting or underfitting and to play with your model hyperparameters/optimizer & loss function.**"
      ],
      "metadata": {
        "id": "NFvV7H7OBzWl"
      },
      "id": "NFvV7H7OBzWl"
    },
    {
      "cell_type": "code",
      "source": [
        "#Pre-training to see what accuracy we can get with random parameters\n",
        "true, pred = val_loop(lstm_model, val_iterator)\n",
        "print()\n",
        "print(f'Binary Macro F1: {binary_macro_f1(true, pred)}')\n",
        "print(f'Accuracy: {accuracy(true, pred)}')"
      ],
      "metadata": {
        "id": "SdkEpedxDopv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8101b27-458e-43b5-db33-58865d85aa19"
      },
      "id": "SdkEpedxDopv",
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 150/150 [00:01<00:00, 141.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Binary Macro F1: 0.5241570853788573\n",
            "Accuracy: 0.54375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Watch the model train!\n",
        "TOTAL_EPOCHS = 10\n",
        "for epoch in range(TOTAL_EPOCHS):\n",
        "    train_loss = train_loop(lstm_model, lstm_criterion, lstm_optimizer, train_iterator)\n",
        "    true, pred = val_loop(lstm_model, val_iterator)\n",
        "    print(f\"EPOCH: {epoch}\")\n",
        "    print(f\"TRAIN LOSS: {train_loss}\")\n",
        "    print(f\"VAL F-1: {binary_macro_f1(true, pred)}\")\n",
        "    print(f\"VAL ACC: {accuracy(true, pred)}\")"
      ],
      "metadata": {
        "id": "6p2dF9X4DyIR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb0b92de-a92c-43a0-cdbb-e17afbd67212"
      },
      "id": "6p2dF9X4DyIR",
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:56<00:00, 21.35it/s]\n",
            "100%|██████████| 150/150 [00:01<00:00, 147.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 0\n",
            "TRAIN LOSS: 18545.684940099716\n",
            "VAL F-1: 0.81704528129288\n",
            "VAL ACC: 0.8258333333333333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:55<00:00, 21.48it/s]\n",
            "100%|██████████| 150/150 [00:01<00:00, 132.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 1\n",
            "TRAIN LOSS: 17683.709224820137\n",
            "VAL F-1: 0.7891576513684857\n",
            "VAL ACC: 0.8095833333333333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:56<00:00, 21.06it/s]\n",
            "100%|██████████| 150/150 [00:01<00:00, 135.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 2\n",
            "TRAIN LOSS: 17453.25639104843\n",
            "VAL F-1: 0.8406084796217621\n",
            "VAL ACC: 0.8445833333333334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:56<00:00, 21.10it/s]\n",
            "100%|██████████| 150/150 [00:01<00:00, 141.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 3\n",
            "TRAIN LOSS: 17257.622393131256\n",
            "VAL F-1: 0.8512003460457069\n",
            "VAL ACC: 0.8566666666666667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:56<00:00, 21.29it/s]\n",
            "100%|██████████| 150/150 [00:01<00:00, 142.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 4\n",
            "TRAIN LOSS: 17191.854114174843\n",
            "VAL F-1: 0.8405219308176581\n",
            "VAL ACC: 0.8445833333333334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:55<00:00, 21.44it/s]\n",
            "100%|██████████| 150/150 [00:01<00:00, 132.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 5\n",
            "TRAIN LOSS: 17090.8236784935\n",
            "VAL F-1: 0.8535276163300628\n",
            "VAL ACC: 0.8608333333333333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:56<00:00, 21.39it/s]\n",
            "100%|██████████| 150/150 [00:01<00:00, 145.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 6\n",
            "TRAIN LOSS: 17034.689063429832\n",
            "VAL F-1: 0.8402548822101624\n",
            "VAL ACC: 0.84375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:56<00:00, 21.37it/s]\n",
            "100%|██████████| 150/150 [00:01<00:00, 145.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 7\n",
            "TRAIN LOSS: 16987.47665476799\n",
            "VAL F-1: 0.8518313946479571\n",
            "VAL ACC: 0.8591666666666666\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:56<00:00, 21.34it/s]\n",
            "100%|██████████| 150/150 [00:01<00:00, 144.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 8\n",
            "TRAIN LOSS: 16938.247215270996\n",
            "VAL F-1: 0.8494184765048163\n",
            "VAL ACC: 0.8554166666666667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:56<00:00, 21.23it/s]\n",
            "100%|██████████| 150/150 [00:01<00:00, 145.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 9\n",
            "TRAIN LOSS: 16880.213525891304\n",
            "VAL F-1: 0.849970619246269\n",
            "VAL ACC: 0.85625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#See how your model does on the held out data\n",
        "true, pred = val_loop(lstm_model, test_iterator)\n",
        "print()\n",
        "print(f\"TEST F-1: {binary_macro_f1(true, pred)}\")\n",
        "print(f\"TEST ACC: {accuracy(true, pred)}\")"
      ],
      "metadata": {
        "id": "OR8Dl5DLEQwd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2118b60-fea0-4149-8289-efb26887ff86"
      },
      "id": "OR8Dl5DLEQwd",
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 150/150 [00:01<00:00, 137.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TEST F-1: 0.9611308611096581\n",
            "TEST ACC: 0.96375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 7: Submit Your Homework\n",
        "This is the end. Congratulations!  \n",
        "\n",
        "Now, follow the steps below to submit your homework in [Gradescope](https://www.gradescope.com/courses/345683):\n",
        "\n",
        "1. Rename this ipynb file to 'CS7650_p1_GTusername.ipynb'. We recommend ensuring you have removed any extraneous cells & print statements, clearing all outputs, and using the Runtime --> Run all tool to make sure all output is update to date. Additionally, leaving comments in your code to help us understand your operations will assist the teaching staff in grading. It is not a requirement, but is recommended. \n",
        "2. Click on the menu 'File' --> 'Download' --> 'Download .py'.\n",
        "3. Click on the menu 'File' --> 'Download' --> 'Download .ipynb'.\n",
        "4. Download the notebook as a .pdf document. Make sure the output from Parts 4 & 6.3 are captured so we can see how the loss, F1, & accuracy changes while training.\n",
        "5. Upload all 3 files to GradeScope.\n"
      ],
      "metadata": {
        "id": "mY8S9ZK9zuVs"
      },
      "id": "mY8S9ZK9zuVs"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}